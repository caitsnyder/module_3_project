{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Submission\n",
    "\n",
    "You'll clean, explore, and model this dataset with a multivariate linear regression to predict the sale price of houses as accurately as possible.\n",
    "\n",
    "Please fill out:\n",
    "* Student name: Caitlin Snyder\n",
    "* Student pace: self-paced\n",
    "* Scheduled project review date/time: \n",
    "* Instructor name: Jeff Herman\n",
    "* Blog post URL: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I wrap up Module 3 of Flat Iron's Data Science bootcamp, I will be \n",
    "\n",
    "...\n",
    "\n",
    "Follow along below, or take a look at the [Jupyter notebook]().\n",
    "\n",
    "https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/page/25/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll try to answer these questions below--let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Import the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the relevant libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class Paths:\n",
    "    def __init__(self):\n",
    "        self.train_values = 'data/training_set_values.csv'\n",
    "        self.train_labels = 'data/training_set_labels.csv'\n",
    "        self.test_values = 'data/test_set_values.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in and preview the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/training_set_values.csv (59400, 39)\n",
      "data/training_set_labels.csv (59400, 1)\n",
      "data/test_set_values.csv (14850, 39)\n"
     ]
    }
   ],
   "source": [
    "class DataKeeper:\n",
    "    def __init__(self):\n",
    "        self.outcome = 'status_group'\n",
    "        paths = Paths()\n",
    "        \n",
    "        X_train = self.load_data(paths.train_values)\n",
    "        y_train = self.load_data(paths.train_labels)\n",
    "        \n",
    "        self.X_test = self.load_data(paths.test_values)\n",
    "        self.train = pd.concat([X_train, y_train], axis=1, join=\"inner\")\n",
    "        \n",
    "    \n",
    "    def load_data(self, path):\n",
    "        df = pd.read_csv(path)\n",
    "        df.set_index('id', inplace=True)\n",
    "        print(path, df.shape)\n",
    "        return df\n",
    "\n",
    "dfs = DataKeeper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the training data, we see that we have 59,400 rows and 40 columns. We also call describe() on the dataframe to get an overview of the descriptive statistics corresponding to each attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfs.train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.plotting.scatter_matrix(dfs.train, figsize=(10,10)); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviewing the data types, we see that there are a number of string columns. We need to examine each of these to see what kind of cleaning may be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfs.train.dtypes\n",
    "\n",
    "# id                         int64\n",
    "# amount_tsh               float64\n",
    "# date_recorded             object\n",
    "# funder                    object\n",
    "# gps_height                 int64\n",
    "# installer                 object\n",
    "# longitude                float64\n",
    "# latitude                 float64\n",
    "# wpt_name                  object\n",
    "# num_private                int64\n",
    "# basin                     object\n",
    "# subvillage                object\n",
    "# region                    object\n",
    "# region_code                int64\n",
    "# district_code              int64\n",
    "# lga                       object\n",
    "# ward                      object\n",
    "# population                 int64\n",
    "# public_meeting            object\n",
    "# recorded_by               object\n",
    "# scheme_management         object\n",
    "# scheme_name               object\n",
    "# permit                    object\n",
    "# construction_year          int64\n",
    "# extraction_type           object\n",
    "# extraction_type_group     object\n",
    "# extraction_type_class     object\n",
    "# management                object\n",
    "# management_group          object\n",
    "# payment                   object\n",
    "# payment_type              object\n",
    "# water_quality             object\n",
    "# quality_group             object\n",
    "# quantity                  object\n",
    "# quantity_group            object\n",
    "# source                    object\n",
    "# source_type               object\n",
    "# source_class              object\n",
    "# waterpoint_type           object\n",
    "# waterpoint_type_group     object\n",
    "# dtype: object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we preview the unique values for each column, we can see that there are common but differently formatted values in the 'funder' and 'installer' columns. For example, 'Brown' appears in column 'funder' where as 'brown' appears in column 'installer.' To align these columns, we'll lowercase both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_str_cols(df):\n",
    "    return df.select_dtypes(include=['object']).columns\n",
    "\n",
    "def show_unique_values(df, cols):\n",
    "    exclude = ['date_recorded']\n",
    "    for col in cols:\n",
    "        if col not in exclude:\n",
    "            print(col)\n",
    "            print(df[col].unique())\n",
    "        \n",
    "def cols_to_lower(df, cols):\n",
    "    for col in cols:\n",
    "        df[col] = df[col].str.lower()\n",
    "        \n",
    "str_cols = get_str_cols(dfs.train)\n",
    "show_unique_values(dfs.train, str_cols)\n",
    "cols_to_lower(dfs.train, ['funder', 'installer'])\n",
    "\n",
    "# funder\n",
    "# ['roman' 'grumeti' 'lottery club' ... 'dina' 'brown' 'samlo']\n",
    "# installer\n",
    "# ['roman' 'grumeti' 'world vision' ... 'dina' 'brown' 'selepta']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to type 'date_recorded' as numeric in order for our ML classifiers to accept the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_date(df, cols):\n",
    "    for col in cols:\n",
    "        df[col] = pd.DatetimeIndex(pd.to_datetime(df[col]))\n",
    "        df[col]=df[col].map(dt.datetime.toordinal)\n",
    "        \n",
    "convert_to_date(dfs.train, ['date_recorded'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining string columns look correctly typed. We'll one-hot encode these values below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have a fair number of null values. For the most part, the percentage of null values is relatively low (< 7%). However, the number of nulls in 'scheme_name' is pretty massive: 47.4%! With nulls making up nearly half of all values in the column, we're better off dropping this feature all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_na_cols(df):\n",
    "    na_sum = df.isna().sum()\n",
    "    na_cols = na_sum[na_sum != 0]\n",
    "    total = df.shape[0]\n",
    "    if len(na_cols) == 0:\n",
    "        print('No nans')\n",
    "    for i in na_cols.index:\n",
    "        cnt = na_cols[i]\n",
    "        pcnt = round((cnt*100)/total, 1)\n",
    "        print(f\"{i}: {cnt} ({pcnt}%)\")\n",
    "\n",
    "        \n",
    "def drop_cols(df, cols):\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            df.drop([col], axis=1, inplace=True)\n",
    "        \n",
    "get_na_cols(dfs.train)\n",
    "\n",
    "# funder: 3635 (6.1%)\n",
    "# installer: 3655 (6.2%)\n",
    "# subvillage: 371 (0.6%)\n",
    "# public_meeting: 3334 (5.6%)\n",
    "# scheme_management: 3877 (6.5%)\n",
    "# scheme_name: 28166 (47.4%)\n",
    "# permit: 3056 (5.1%)\n",
    "\n",
    "drop_cols(dfs.train, ['scheme_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dataset also contains \"unknown\" values. Let's consolidate the nans and unknown values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_unknowns_nan(df):\n",
    "    df.replace('unknown', np.nan, inplace=True)\n",
    "    \n",
    "make_unknowns_nan(dfs.train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ohe(df):\n",
    "    ohe = OneHotEncoder()\n",
    "    ohe.fit(df)\n",
    "    return ohe.transform(df).toarray()\n",
    "\n",
    "ohe_train = get_ohe(dfs.train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the mean and standard deviation associated with outcome type (functionality):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggs = dfs.train.groupby(dfs.train[dfs.outcome]).agg(['mean', 'std'])\n",
    "aggs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can first define a class to split our data into training and testing subsets and peform the necessary transformations on our continuous and categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidate our data cleaning steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can update our DataKeeper class and add additional helper classes to perform all of the above data cleaning steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousHelper():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_cleaned_df(self, df, features):\n",
    "        self.impute_zeros(df, ['construction_year', 'population'])\n",
    "        list(map(lambda col: self.handle_zeros_pre_log(df, col), features))\n",
    "        list(map(lambda col: self.handle_negatives_pre_log(df, col), features))\n",
    "        \n",
    "    def impute_zeros(self, df, cols):\n",
    "        for col in cols:\n",
    "            df[col] = df.apply(\n",
    "                lambda row: np.nan if row[col] == 0 else row[col], axis=1)\n",
    "            self.impute_median(df, col)\n",
    "    \n",
    "    def impute_median(self, df, col):\n",
    "        df_one_col = df[[col]]\n",
    "        df_one_col.fillna(df_one_col.median(), inplace=True)        \n",
    "        df[col] = df_one_col[col]\n",
    "            \n",
    "    def handle_zeros_pre_log(self, df, col):\n",
    "        if df[col].min() != 0:\n",
    "            return\n",
    "        elif (df[col].min() == 0 and df[col].max() == 0):\n",
    "            df.drop([col],axis=1, inplace=True)\n",
    "        else:\n",
    "            col_non_zero_min = df.loc[df[col] > 0, col].min()\n",
    "            offset = col_non_zero_min/2\n",
    "            df[col] = df.apply(\n",
    "                lambda row: row[col] + offset,\n",
    "                axis=1)\n",
    "            \n",
    "    def handle_negatives_pre_log(self, df, col):\n",
    "        if df[col].min() >= 0:\n",
    "            return\n",
    "        else:\n",
    "            col_min = abs(df[col].min()) + 1\n",
    "            df[col] = df.apply(\n",
    "                lambda row: row[col] + col_min,\n",
    "                axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FuzzyMatcher:\n",
    "    def __init__(self):\n",
    "        self.cols = []\n",
    "    \n",
    "    def clean_orgs(self, df, cols):\n",
    "        self.cols = cols\n",
    "        self.clean_col_text(df)\n",
    "        \n",
    "        scores_df = self.get_matches(df)       \n",
    "        self.replace_with_matches(df, scores_df) \n",
    "        \n",
    "    def clean_col_text(self, df):\n",
    "        for col in self.cols:\n",
    "            df[col] = df[col].str.lower()\n",
    "            df[col] = df.apply(lambda row: self.manually_clean_col_text(str(row[col])), axis=1)\n",
    "            df[col] = df.apply(lambda row: self.remove_special_chars(str(row[col])), axis=1)\n",
    "            \n",
    "            \n",
    "    def manually_clean_col_text(self, value):\n",
    "        replace_dict = { # ideally in constants file\n",
    "            \"private individual\": \"private\",\n",
    "            \"not known\": \"unknown\",\n",
    "            \"0\": \"unknown\",\n",
    "            \"-\": \"unknown\",\n",
    "            \"nan\": \"unknown\",\n",
    "            \"action in a\": \"action in africa\",\n",
    "            \"wateraid\": \"water aid\"\n",
    "        }\n",
    "        if value in replace_dict.keys():\n",
    "            value = replace_dict[value]\n",
    "        return value\n",
    "    \n",
    "    def remove_special_chars(self, value):\n",
    "        special_chars = [\".\", \"/\", \"-\", \"[\", \"]\", \"(\", \")\"]\n",
    "        for char in special_chars:\n",
    "            value = value.replace(char, \"\")\n",
    "        return value\n",
    "    \n",
    "    def get_matches(self, df):\n",
    "        values = self.get_orgs(df)\n",
    "        match_df = self.get_match_df(values)\n",
    "        scores_df = self.get_scores(match_df)\n",
    "        return scores_df\n",
    "\n",
    "    def get_orgs(self, df):\n",
    "        list_of_lists = [df[col] for col in self.cols]\n",
    "        orgs = [j for sub in list_of_lists for j in sub]\n",
    "        return np.unique([str(i).lower() for i in orgs if str(i) != 'nan']).tolist()\n",
    "    \n",
    "    def get_match_df(self, values):\n",
    "        score_sort = [(x,) + i\n",
    "                     for x in values \n",
    "                     for i in process.extract(x, values, scorer=fuzz.token_sort_ratio)]\n",
    "        match_df = pd.DataFrame(score_sort, columns=['name_sort','match_sort','score_sort'])\n",
    "        match_df['sorted_name_sort'] = \\\n",
    "            np.minimum(match_df['name_sort'], match_df['match_sort'])\n",
    "        return match_df\n",
    "    \n",
    "    def get_scores(self, match_df):\n",
    "        high_score_sort = self.get_score_sort(match_df)\n",
    "        scores = self.get_score_groups(high_score_sort)\n",
    "        scores_df = self.get_score_frame(scores)\n",
    "        return scores_df\n",
    "\n",
    "    def get_score_sort(self, match_df):\n",
    "        high_score_sort = \\\n",
    "            match_df.loc[(match_df['score_sort'] >= 80) &\n",
    "                    (match_df['name_sort'] !=  match_df['match_sort']) &\n",
    "                    (match_df['sorted_name_sort'] != match_df['match_sort'])]\n",
    "        \n",
    "        high_score_sort = high_score_sort.drop('sorted_name_sort', axis=1).copy()\n",
    "        return high_score_sort\n",
    "        \n",
    "\n",
    "    def get_score_groups(self, high_score_sort):\n",
    "        return high_score_sort.groupby(['name_sort','score_sort']).agg(\n",
    "                        {'match_sort': ', '.join}).sort_values(\n",
    "                        ['score_sort'], ascending=False)\n",
    "        \n",
    "    def get_score_frame(self, scores):\n",
    "        frame = { \n",
    "            'name_sort': scores.index.get_level_values(0), \n",
    "            'score_sort': scores.index.get_level_values(1),\n",
    "            'match_sort': list(map(lambda x: x[0], scores.values.tolist())),\n",
    "        }\n",
    "        return pd.DataFrame(frame)\n",
    "    \n",
    "    def replace_with_matches(self, df, df_matches):\n",
    "        for col in self.cols:\n",
    "            for name in df_matches[\"name_sort\"]:\n",
    "                if name in df[col].values.tolist():\n",
    "                    replace_value = df_matches.loc[df_matches[\"name_sort\"] == name,\n",
    "                                                   \"match_sort\"].values.tolist()[0]\n",
    "                    df[col].replace(name, replace_value, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalHelper():\n",
    "    def __init__(self):\n",
    "        self.fuzzy_matcher = FuzzyMatcher()\n",
    "    \n",
    "    def get_cleaned_df(self, df, features):\n",
    "        df.fillna('unknown', inplace=True) \n",
    "        \n",
    "    def match_names(self, df, cols):\n",
    "        for col in cols:\n",
    "            pass\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cleaner():\n",
    "    def __init__(self):\n",
    "        self.cont_helper = ContinuousHelper()\n",
    "        self.cat_helper = CategoricalHelper()\n",
    "        \n",
    "    def get_cleaned_df(self, df):\n",
    "        self.general_cleaning(df)\n",
    "        self.cat_helper.get_cleaned_df(df, self.get_cat_features(df))\n",
    "        self.cont_helper.get_cleaned_df(df, self.get_cont_features(df))\n",
    "        return df\n",
    "    \n",
    "    def general_cleaning(self, df):\n",
    "        self.convert_to_date(df, ['date_recorded'])\n",
    "        self.drop_cols(df, ['scheme_name', 'date_recorded']) # why dropping date recorded?\n",
    "        \n",
    "    def convert_to_date(self, df, cols):\n",
    "        for col in cols:\n",
    "            df[col] = pd.DatetimeIndex(pd.to_datetime(df[col]))\n",
    "            df[col] = df[col].map(dt.datetime.toordinal)\n",
    "    \n",
    "    def drop_cols(self, df, cols):\n",
    "        for col in cols:\n",
    "            if col in df.columns:\n",
    "                df.drop([col], axis=1, inplace=True)\n",
    "                \n",
    "    def get_cat_features(self, df):\n",
    "        self.change_type(df, 'region_code', str)\n",
    "        self.change_type(df, 'district_code', str)\n",
    "        self.change_type(df, 'num_private', str)\n",
    "        return df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    def change_type(self, df, col, new_type):\n",
    "        df[col] = df[col].astype(new_type)\n",
    "        \n",
    "    def get_cont_features(self, df):\n",
    "        return df.select_dtypes(exclude=['object']).columns\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Splitter:\n",
    "    def __init__(self):\n",
    "        self.split_dfs = {'X_train': None, 'X_test': None, 'y_train': None, 'y_test': None}\n",
    "        \n",
    "    def get_splits(self, df, outcome):\n",
    "        self.set_splits(df, outcome)\n",
    "        return self.split_dfs\n",
    "    \n",
    "    def set_splits(self, df, outcome):\n",
    "        preds = [i for i in df.columns if i != outcome]\n",
    "        X = df[preds]\n",
    "        y = df[[outcome]]\n",
    "        \n",
    "        self.split_dfs['X_train'], self.split_dfs['X_test'], self.split_dfs['y_train'], self.split_dfs['y_test'] =\\\n",
    "            train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "        \n",
    "        list(map(lambda key: self.apply_transformations(key), ['X_train', 'X_test']))\n",
    "        self.match_columns()\n",
    "            \n",
    "            \n",
    "    def apply_transformations(self, key):\n",
    "        print(key, ': in')\n",
    "        \n",
    "        df = self.split_dfs[key]\n",
    "        features = self.get_feature_types(df)\n",
    "        \n",
    "        df_cat = self.transform_cat(df[features['cat']])\n",
    "        df_cont = self.transform_cont(df[features['cont']])\n",
    "                \n",
    "        merged_df = pd.concat([df_cat, df_cont], axis=1)\n",
    "        self.split_dfs[key] = merged_df\n",
    "        \n",
    "        print(key, ': out')\n",
    "\n",
    "    def get_feature_types(self, df):\n",
    "        cat_features = df.select_dtypes(include=['object']).columns\n",
    "        cont_features = df.select_dtypes(exclude=['object']).columns\n",
    "        return {'cat': cat_features, 'cont': cont_features}\n",
    "        \n",
    "    def transform_cont(self, df):\n",
    "        df_log = np.log(df)\n",
    "        df_log.columns = [f'{column}_log' for column in df.columns]\n",
    "        return df_log.apply(self.normalize)\n",
    "    \n",
    "    def normalize(self, feature):\n",
    "        return (feature - feature.mean()) / feature.std()\n",
    "\n",
    "    def transform_cat(self, df_cat):\n",
    "        df_cat = df_cat.astype(str)\n",
    "        id_index = df_cat.index\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "        ohe.fit(df_cat)\n",
    "        \n",
    "        matrix_ohe = ohe.transform(df_cat).toarray()\n",
    "        df_ohe = pd.DataFrame(matrix_ohe, columns=ohe.get_feature_names(df_cat.columns))\n",
    "        df_ohe['id'] = id_index\n",
    "        df_ohe.set_index('id', drop=True, inplace=True)\n",
    "        return df_ohe\n",
    "    \n",
    "    def match_columns(self):\n",
    "        train_cols = self.split_dfs['X_train'].columns.values.tolist()\n",
    "        test_cols = self.split_dfs['X_test'].columns.values.tolist()\n",
    "        print(0)\n",
    "        self.add_match_cols(train_cols, test_cols, 'X_test')\n",
    "        print(1)\n",
    "        self.add_match_cols(test_cols, train_cols, 'X_train')\n",
    "        print(2)\n",
    "        \n",
    "    def add_match_cols(self, source_cols, target_cols, target_df_key):\n",
    "        for col in source_cols:\n",
    "            if col not in target_cols:\n",
    "                print(col)\n",
    "                self.split_dfs[target_df_key][col] = 0\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelReport:\n",
    "    def __init__(self):\n",
    "        self.splits = None\n",
    "        self.accuracy_report = {'tree': 0, 'svm': 0, 'knn': 0}\n",
    "    \n",
    "    def get_reports(self, splits):\n",
    "        self.splits = splits\n",
    "        self.get_decision_tree_report()\n",
    "        self.get_svm_report()\n",
    "        self.get_knn_report()\n",
    "        self.display_results()\n",
    "        \n",
    "    def get_decision_tree_report(self):\n",
    "        clf = DecisionTreeClassifier(criterion='entropy')\n",
    "        clf.fit(self.splits['X_train'], self.splits['y_train'])\n",
    "        y_pred_tree = clf.predict(self.splits['X_test'])\n",
    "        self.accuracy_report['tree'] = accuracy_score(self.splits['y_test'], y_pred_tree)\n",
    "\n",
    "    def show_decision_tree_plot(self, outcome_values): # Not actively used\n",
    "        features = self.get_features()\n",
    "        fig, axes = plt.subplots(nrows = 1,ncols = 1, figsize = (3,3), dpi=300)\n",
    "        tree.plot_tree(clf,\n",
    "                       feature_names=features, \n",
    "                       class_names=outcome_values.astype('str'),\n",
    "                       filled = True)\n",
    "        plt.show()\n",
    "        \n",
    "    def get_features(self):\n",
    "        self.features = self.splits['X_train'].columns.values.tolist() + \\\n",
    "            self.splits['y_train'].columns.values.tolist()\n",
    "        \n",
    "    def get_svm_report(self):\n",
    "        SVC_model = SVC()\n",
    "        SVC_model.fit(self.splits['X_train'], self.splits['y_train'])\n",
    "        SVC_prediction = SVC_model.predict(self.splits['X_test'])\n",
    "        self.accuracy_report['svm'] = accuracy_score(SVC_prediction, self.splits['y_test'])\n",
    "    \n",
    "    def get_knn_report(self):\n",
    "        KNN_model = KNeighborsClassifier(n_neighbors=5)\n",
    "        KNN_model.fit(self.splits['X_train'], self.splits['y_train'])\n",
    "        KNN_prediction = KNN_model.predict(self.splits['X_test'])\n",
    "#         print(confusion_matrix(SVC_prediction, dfs.splits['y_test']))\n",
    "#         print(classification_report(KNN_prediction, dfs.splits['y_test']))\n",
    "        self.accuracy_report['knn'] = accuracy_score(KNN_prediction, self.splits['y_test'])\n",
    "\n",
    "    def display_results(self):\n",
    "        print('---------\\nAccuracy reports\\n---------')\n",
    "        for key in self.accuracy_report.keys():\n",
    "            print(key, self.accuracy_report[key])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataKeeper:\n",
    "    def __init__(self, run_type_dev=False):\n",
    "        self.run_type_dev = run_type_dev\n",
    "        self.outcome = 'status_group'\n",
    "        self.outcome_values = []\n",
    "        \n",
    "        self.cleaned_df = None\n",
    "        self.splits = None\n",
    "        \n",
    "        self.paths = Paths()\n",
    "        self.cleaner = Cleaner()\n",
    "        self.splitter = Splitter()\n",
    "        self.report = ModelReport()\n",
    "        \n",
    "        self.get_data()\n",
    "\n",
    "    def get_data(self):\n",
    "        df = self.load_df()\n",
    "        \n",
    "        self.fuzzy_matcher.clean_orgs(df, ['funder', 'installer'])\n",
    "        self.preprocess(df)\n",
    "        \n",
    "    def load_df(self):\n",
    "        X_train = self.load_data(self.paths.train_values)\n",
    "        y_train = self.load_data(self.paths.train_labels)\n",
    "        \n",
    "        if self.run_type_dev:\n",
    "            X_train = X_train.iloc[0:50]\n",
    "            y_train = y_train.iloc[0:50]\n",
    "            \n",
    "        df = pd.concat([X_train, y_train], axis=1, join=\"inner\")\n",
    "        self.outcome_values = np.unique(df[self.outcome])\n",
    "        return df\n",
    "    \n",
    "    def load_data(self, path):\n",
    "        df = pd.read_csv(path)\n",
    "        df.set_index('id', inplace=True)\n",
    "        return df\n",
    "        \n",
    "    def preprocess(self, raw_df):\n",
    "        self.cleaned_df = self.cleaner.get_cleaned_df(raw_df)\n",
    "        self.splits = self.splitter.get_splits(self.cleaned_df, self.outcome)\n",
    "        \n",
    "    def get_report(self):\n",
    "        self.report.get_reports(self.splits)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train : in\n",
      "X_train : out\n",
      "X_test : in\n",
      "X_test : out\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "dfs = DataKeeper(run_type_dev=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.get_report()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://towardsdatascience.com/fuzzywuzzy-find-similar-strings-within-one-column-in-a-pandas-data-frame-99f6c2a0c212\n",
    "    \n",
    "https://towardsdatascience.com/fuzzywuzzy-fuzzy-string-matching-in-python-beginners-guide-9adc0edf4b35\n",
    "\n",
    "https://stackabuse.com/overview-of-classification-methods-in-python-with-scikit-learn/\n",
    "    \n",
    "https://stackabuse.com/the-naive-bayes-algorithm-in-python-with-scikit-learn/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
