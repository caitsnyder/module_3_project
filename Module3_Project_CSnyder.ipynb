{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Submission\n",
    "\n",
    "You'll clean, explore, and model this dataset with a multivariate linear regression to predict the sale price of houses as accurately as possible.\n",
    "\n",
    "Please fill out:\n",
    "* Student name: Caitlin Snyder\n",
    "* Student pace: self-paced\n",
    "* Scheduled project review date/time: \n",
    "* Instructor name: Jeff Herman\n",
    "* Blog post URL: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I wrap up Module 3 of Flat Iron's Data Science bootcamp, I will be \n",
    "\n",
    "...\n",
    "\n",
    "Follow along below, or take a look at the [Jupyter notebook]().\n",
    "\n",
    "https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/page/25/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll try to answer these questions below--let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Define the relevant classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll be taking an object-oriented approach to this project and will begin by defining the classes and constants I'll need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Accessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths\n",
    "We'll define our path strings within a simple dictionary for easy loading:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {\n",
    "        'train_values': 'data/training_set_values.csv',\n",
    "        'train_labels': 'data/training_set_labels.csv',\n",
    "        'test_values': 'data/test_set_values.csv'\n",
    "}       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader\n",
    "The Data Loader will load the appropriate csvs. This helper class includes an option (run_type_dev) to downsample our dataset as needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def load(self, outcome, run_type_dev, sample_size):\n",
    "        X_train = self.load_from_path(paths['train_values'])\n",
    "        y_train = self.load_from_path(paths['train_labels'])\n",
    "        \n",
    "        if run_type_dev:\n",
    "            print(f\"Sample size = {sample_size}\")\n",
    "            X_train = X_train.iloc[0:sample_size]\n",
    "            y_train = y_train.iloc[0:sample_size]\n",
    "            \n",
    "        df = pd.concat([X_train, y_train], axis=1, join=\"inner\")\n",
    "        self.outcome_values = np.unique(df[outcome])\n",
    "        return df\n",
    "    \n",
    "    def load_from_path(self, path):\n",
    "        df = pd.read_csv(path)\n",
    "        df.set_index('id', inplace=True)\n",
    "        return df\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy Matcher\n",
    "Columns 'funder' and 'installer' include many similar values with slightly different spellings (ie, 'unicef' and 'unicf'). We'll use the Fuzzy Matcher class to consolidate these values using the fuzzy-wuzzy library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "\n",
    "class FuzzyMatcher:\n",
    "    def __init__(self):\n",
    "        self.cols = []\n",
    "    \n",
    "    def set_fuzzy_matches(self, df, cols):\n",
    "        self.cols = cols\n",
    "        self.clean_col_text(df)\n",
    "        \n",
    "        scores_df = self.get_matches(df)       \n",
    "        self.replace_with_matches(df, scores_df) \n",
    "        \n",
    "    def clean_col_text(self, df):\n",
    "        for col in self.cols:\n",
    "            df[col] = df[col].str.lower()\n",
    "            df[col] = df.apply(lambda row: self.manually_clean_col_text(str(row[col])), axis=1)\n",
    "            df[col] = df.apply(lambda row: self.remove_special_chars(str(row[col])), axis=1)\n",
    "            \n",
    "            \n",
    "    def manually_clean_col_text(self, value):\n",
    "        replace_dict = { # ideally in constants file\n",
    "            \"private individual\": \"private\",\n",
    "            \"not known\": \"unknown\",\n",
    "            \"0\": \"unknown\",\n",
    "            \"-\": \"unknown\",\n",
    "            \"nan\": \"unknown\",\n",
    "            \"action in a\": \"action in africa\",\n",
    "            \"wateraid\": \"water aid\"\n",
    "        }\n",
    "        if value in replace_dict.keys():\n",
    "            value = replace_dict[value]\n",
    "        return value\n",
    "    \n",
    "    def remove_special_chars(self, value):\n",
    "        special_chars = [\".\", \"/\", \"-\", \"[\", \"]\", \"(\", \")\"]\n",
    "        for char in special_chars:\n",
    "            value = value.replace(char, \"\")\n",
    "        return value\n",
    "    \n",
    "    def get_matches(self, df):\n",
    "        values = self.get_values(df)\n",
    "        match_df = self.get_match_df(values)\n",
    "        scores_df = self.get_scores(match_df)\n",
    "        return scores_df\n",
    "\n",
    "    def get_values(self, df):\n",
    "        list_of_lists = [df[col] for col in self.cols]\n",
    "        values = [j for sub in list_of_lists for j in sub]\n",
    "        return np.unique([str(i).lower() for i in values if str(i) != 'nan']).tolist()\n",
    "    \n",
    "    def get_match_df(self, values):\n",
    "        score_sort = [(x,) + i\n",
    "                     for x in values \n",
    "                     for i in process.extract(x, values, scorer=fuzz.token_sort_ratio)]\n",
    "        match_df = pd.DataFrame(score_sort, columns=['name_sort','match_sort','score_sort'])\n",
    "        match_df['sorted_name_sort'] = \\\n",
    "            np.minimum(match_df['name_sort'], match_df['match_sort'])\n",
    "        return match_df\n",
    "    \n",
    "    def get_scores(self, match_df):\n",
    "        high_score_sort = self.get_score_sort(match_df)\n",
    "        scores = self.get_score_groups(high_score_sort)\n",
    "        scores_df = self.get_score_frame(scores)\n",
    "        return scores_df\n",
    "\n",
    "    def get_score_sort(self, match_df):\n",
    "        high_score_sort = \\\n",
    "            match_df.loc[(match_df['score_sort'] >= 80) &\n",
    "                    (match_df['name_sort'] !=  match_df['match_sort']) &\n",
    "                    (match_df['sorted_name_sort'] != match_df['match_sort'])]\n",
    "        \n",
    "        high_score_sort = high_score_sort.drop('sorted_name_sort', axis=1).copy()\n",
    "        return high_score_sort\n",
    "        \n",
    "\n",
    "    def get_score_groups(self, high_score_sort):\n",
    "        return high_score_sort.groupby(['name_sort','score_sort']).agg(\n",
    "                        {'match_sort': ', '.join}).sort_values(\n",
    "                        ['score_sort'], ascending=False)\n",
    "        \n",
    "    def get_score_frame(self, scores):\n",
    "        frame = { \n",
    "            'name_sort': scores.index.get_level_values(0), \n",
    "            'score_sort': scores.index.get_level_values(1),\n",
    "            'match_sort': list(map(lambda x: x[0], scores.values.tolist())),\n",
    "        }\n",
    "        return pd.DataFrame(frame)\n",
    "    \n",
    "    def replace_with_matches(self, df, df_matches):\n",
    "        for col in self.cols:\n",
    "            for name in df_matches[\"name_sort\"]:\n",
    "                if name in df[col].values.tolist():\n",
    "                    replace_value = df_matches.loc[df_matches[\"name_sort\"] == name,\n",
    "                                                   \"match_sort\"].values.tolist()[0]\n",
    "                    df[col].replace(name, replace_value, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VizHelper\n",
    "The Viz Helper will output relevant visualizations to inform iterative cleaning and analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics._plot.confusion_matrix import plot_confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "class VizHelper:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def show_visualizations(self, df, outcome):\n",
    "        cont_features = df.select_dtypes(exclude=['object']).columns\n",
    "        self.generate_heat_map(df, cont_features)\n",
    "        self.show_outliers(df, cont_features)\n",
    "        self.show_basic_correlations(df, cont_features, outcome)\n",
    "        self.show_outcome_dist(df, outcome)\n",
    "\n",
    "    def generate_heat_map(self, df, features):\n",
    "        plt.figure(figsize=(7, 6))\n",
    "        sns.heatmap(df[features].corr(), center=0)\n",
    "        plt.show()\n",
    "\n",
    "    def show_outliers(self, df, cols):\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(9, 6))\n",
    "        axe = axes.ravel()\n",
    "\n",
    "        for i, xcol in enumerate(cols):\n",
    "            sns.boxplot(x=df[xcol], ax=axe[i])\n",
    "        plt.show()\n",
    "\n",
    "    def show_basic_correlations(self, df, cols, outcome):\n",
    "        preds = [i for i in cols if i != outcome]\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(9, 6))\n",
    "        axe = axes.ravel()\n",
    "\n",
    "        for i, xcol in enumerate(preds):\n",
    "            df.plot(kind='scatter', x=xcol, y=outcome, alpha=0.4, color='b', ax=axe[i])\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    def show_outcome_dist(self, df, outcome):\n",
    "        df[outcome].value_counts().plot(kind='bar')\n",
    "\n",
    "    def show_confusion_matrix(self, clf, X_test, y_test, outcome, title):\n",
    "        labels = y_test[outcome].unique()\n",
    "        disp = plot_confusion_matrix(clf, X_test, y_test,\n",
    "                                        display_labels=labels,\n",
    "                                        cmap=plt.cm.Greens,\n",
    "                                        xticks_rotation='vertical')\n",
    "        disp.ax_.set_title(title)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaner\n",
    "Our Cleaner will call the Fuzzy Matcher class as well as perform some basic cleaning tasks (eliminate impossible 0 values, correct column data types) on our raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "class Cleaner():\n",
    "    def __init__(self):\n",
    "        self.viz_helper = VizHelper()\n",
    "\n",
    "    def clean_df(self, df):\n",
    "        df.drop(['num_private', 'amount_tsh'], axis=1, inplace=True)\n",
    "        self.convert_to_string(df)\n",
    "        self.replace_nan(df)\n",
    "        FuzzyMatcher().set_fuzzy_matches(df, ['funder', 'installer'])\n",
    "        return df\n",
    "    def convert_to_string(self, df):\n",
    "        cols = [\n",
    "            'region_code', \n",
    "            'district_code', \n",
    "            'public_meeting', \n",
    "            'permit'\n",
    "        ]\n",
    "        list(map(lambda col: self.change_type(df, col, str), cols))\n",
    "    \n",
    "    def change_type(self, df, col, new_type):\n",
    "        df[col] = df[col].astype(new_type)\n",
    "\n",
    "    def replace_nan(self, df):\n",
    "        self.replace_null_strings(df, \"nan\")\n",
    "        self.replace_null_strings(df, \"none\")\n",
    "        self.replace_zeros(df, \"longitude\")\n",
    "        self.replace_zeros(df, \"latitude\")\n",
    "        self.replace_zeros(df, 'construction_year')\n",
    "        self.replace_zeros(df, 'population')\n",
    "\n",
    "    def replace_null_strings(self, df, null_str):\n",
    "        df.replace(to_replace=null_str, value=\"unknown\", inplace=True)\n",
    "\n",
    "    def replace_zeros(self, df, col):\n",
    "        df[col] = df.apply(lambda row: np.nan \n",
    "            if row[col] == 0 else row[col], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) Build the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splits Manager\n",
    "The Splits Manager will allow us to easily access our train-test datasets (without worrying about the typos that a simple dictionary access command is susceptible to): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitsManager:\n",
    "    def __init__(self):\n",
    "        self.X_train = None\n",
    "        self.X_test = None, \n",
    "        self.y_train = None \n",
    "        self.y_test = None\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processor\n",
    "The Pre-Processor defines transformations our pipeline will use. Since our dataset is unbalanced (a histogram of our outcome variable shows that 'functional' is vastly over-represented in the dataset relative to the other two outcomes), we will oversample our minority outcomes to achieve a more balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "class PreProcessor():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_preprocessor(self, df):\n",
    "        return {\n",
    "            'sampler': self.get_resampler(),\n",
    "            'col': self.get_col_transformer(df)\n",
    "            }\n",
    "\n",
    "    def get_resampler(self):\n",
    "        return RandomOverSampler(random_state=42)\n",
    "        \n",
    "    def get_col_transformer(self, df):\n",
    "        cont_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())])\n",
    "\n",
    "        cat_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "        return ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', cont_transformer, self.get_cont_features(df)),\n",
    "                ('cat', cat_transformer, self.get_cat_features(df))])\n",
    "\n",
    "    def get_cat_features(self, df):\n",
    "        return df.select_dtypes(include=['object']).columns\n",
    "        \n",
    "    def get_cont_features(self, df):\n",
    "        return df.select_dtypes(exclude=['object']).columns\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers\n",
    "We can store our classifiers in a dictionary so that we can easily iterate over them during analysis:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "classifiers = {\n",
    "    'decision_tree': DecisionTreeClassifier(),\n",
    "    'random_forest': RandomForestClassifier(),\n",
    "    'svm': SVC(),\n",
    "    'knn': KNeighborsClassifier(),\n",
    "    'xgboost': XGBClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Param Grids\n",
    "We can similarly store the corresponding param grid for each classifier in a dictionary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "param_grids = {\n",
    "    'decision_tree': {\n",
    "        'clf__criterion': ['gini', 'entropy'],\n",
    "        'clf__max_depth': [None, 2, 3, 4, 5, 6],\n",
    "        'clf__min_samples_split': [2, 5, 10],\n",
    "        'clf__min_samples_leaf': [1, 2, 3, 4, 5, 6]\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'clf__n_estimators': [10, 20, 30],\n",
    "        'clf__criterion': ['gini', 'entropy'],\n",
    "        'clf__max_depth': [5, 10, 20, 30, 35],\n",
    "        'clf__min_samples_split': [2, 5, 10],\n",
    "        'clf__min_samples_leaf': [2, 3, 6]\n",
    "    },\n",
    "    'svm': {\n",
    "        'clf__C': [0.1, 1, 10, 100, 1000], \n",
    "        'clf__gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "        'clf__kernel': ['rbf']\n",
    "    },\n",
    "    'knn': {\n",
    "        'clf__n_neighbors': [3, 5, 7, 11, 19],\n",
    "        'clf__weights': ['uniform', 'distance'],\n",
    "        'clf__metric': ['euclidean', 'manhattan']\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'clf__max_depth':6,\n",
    "        'clf__min_child_weight': 1,\n",
    "        'clf__eta':.3,\n",
    "        'clf__subsample': 1,\n",
    "        'clf__colsample_bytree': 1,\n",
    "        'clf__objective':'reg:linear',\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report Manager\n",
    "The Report Manager will use the shared keys in these two dictionaries to iterate over each classifier and score its performance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "\n",
    "\n",
    "class ReportManager:\n",
    "    def __init__(self, outcome):\n",
    "        self.splits = None\n",
    "        self.outcome = outcome\n",
    "        self.results = []\n",
    "        \n",
    "    def run_reports(self, preprocessor, splits: SplitsManager):\n",
    "        self.splits = splits\n",
    "        list(map(lambda key: self.execute_pipeline(preprocessor, key), classifiers.keys()))\n",
    "        list(map(lambda x: self.display_predictions(x), self.results))\n",
    "        self.display_results()\n",
    "\n",
    "    def execute_pipeline(self, preprocessor, key):\n",
    "        print(f\"Beginning {key}...\")\n",
    "        param_grid = param_grids[key]\n",
    "        param_grid['col__num__imputer__strategy'] = ['mean', 'median']\n",
    "\n",
    "        pipe = imbPipeline([\n",
    "            ('col', preprocessor['col']),\n",
    "            ('sampler', preprocessor['sampler']),\n",
    "            (\"clf\", classifiers[key])\n",
    "            ])\n",
    "        gs = RandomizedSearchCV(pipe,\n",
    "                            param_grid,\n",
    "                            cv=3,\n",
    "                            scoring=\"accuracy\",\n",
    "                            n_jobs=-1\n",
    "                        )\n",
    "        gs_results  = gs.fit(self.splits.X_train, self.splits.y_train.values.ravel())\n",
    "        self.results.append(ResultsManager(key, gs_results))\n",
    "\n",
    "    def display_results(self):\n",
    "        rows = list(map(lambda x: [x.clf_name, x.best_score, x.best_params], self.results))\n",
    "        df = pd.DataFrame(rows, columns=[\n",
    "            'clf_name', \n",
    "            'best_score', \n",
    "            'best_params'\n",
    "        ]).sort_values(by='best_score', ascending = False)\n",
    "        print(df)\n",
    "        print('---------\\nAccuracy reports\\n---------')\n",
    "        for result in self.results:\n",
    "            print(f\"{result.clf_name}: {result.best_score:.2%}\")\n",
    "\n",
    "    def display_predictions(self, result: ResultsManager):\n",
    "        title = f\"{result.clf_name} ({result.best_score:.2%})\"\n",
    "        VizHelper().show_confusion_matrix(result.best_estimator, self.splits.X_test, self.splits.y_test, self.outcome, title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D) Tying it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Manager\n",
    "The Data Manager is responsible for coordinating the different helper classes responsible for cleaning, pipeline-creation, visualization, and analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "class DataManager:\n",
    "    def __init__(self, sample_size, run_type_dev=True):\n",
    "        self.sample_size = sample_size\n",
    "        self.outcome = 'status_group'\n",
    "        self.splits = SplitsManager()\n",
    "        self.process_data(run_type_dev,)\n",
    "\n",
    "    def process_data(self, run_type_dev):\n",
    "        raw_df = DataLoader().load(self.outcome, run_type_dev, self.sample_size)\n",
    "        df = Cleaner().clean_df(raw_df)\n",
    "        # VizHelper().show_visualizations(df, self.outcome)\n",
    "        self.set_splits(df, self.outcome)\n",
    "   \n",
    "    def set_splits(self, df, outcome):\n",
    "        X = df[[i for i in df.columns if i != outcome]]\n",
    "        y = df[[outcome]]\n",
    "        \n",
    "        self.splits.X_train, self.splits.X_test,\\\n",
    "            self.splits.y_train, self.splits.y_test =\\\n",
    "            tts(X, y, test_size = 0.2, random_state = 42)\n",
    "            \n",
    "    def get_report(self):\n",
    "        preprocessor = PreProcessor().get_preprocessor(self.splits.X_train)\n",
    "        ReportManager(self.outcome).run_reports(preprocessor, self.splits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined our classes, we can kick off the analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = DataManager(sample_size=500, run_type_dev=True)\n",
    "dfs.get_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Results & Take-aways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviewing both the classifier score readings and the confusion matrices outputted by our ReportManager class, we can see that the Random Forest Classifier has performed best (with an accuracy score of )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional areas for exploration might include elevation and water-resource conflict incidences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://towardsdatascience.com/fuzzywuzzy-find-similar-strings-within-one-column-in-a-pandas-data-frame-99f6c2a0c212\n",
    "    \n",
    "- https://towardsdatascience.com/fuzzywuzzy-fuzzy-string-matching-in-python-beginners-guide-9adc0edf4b35\n",
    "\n",
    "- https://stackabuse.com/overview-of-classification-methods-in-python-with-scikit-learn/\n",
    "    \n",
    "- https://stackabuse.com/the-naive-bayes-algorithm-in-python-with-scikit-learn/\n",
    "\n",
    "- https://medium.com/@erikgreenj/k-neighbors-classifier-with-gridsearchcv-basics-3c445ddeb657\n",
    "\n",
    "- https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf\n",
    "\n",
    "- https://datascience.stackexchange.com/questions/60862/if-i-have-negative-and-positive-numbers-for-a-feature-should-minmaxscaler-be-1\n",
    "\n",
    "- https://stackoverflow.com/questions/41899132/invalid-parameter-for-sklearn-estimator-pipeline\n",
    "\n",
    "- https://lifewithdata.com/2021/04/02/how-to-build-machine-learning-pipeline-with-scikit-learn-and-why-is-it-essential/\n",
    "\n",
    "- https://stackoverflow.com/questions/63467815/how-to-access-columntransformer-elements-in-gridsearchcv\n",
    "\n",
    "- https://openscoring.io/blog/2020/10/24/converting_sklearn_imblearn_pipeline_pmml/\n",
    "\n",
    "- https://imbalanced-learn.org/stable/over_sampling.html\n",
    "\n",
    "- https://towardsdatascience.com/imbalanced-class-sizes-and-classification-models-a-cautionary-tale-part-2-cf371500d1b3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
