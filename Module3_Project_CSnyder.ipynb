{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Submission\n",
    "\n",
    "You'll clean, explore, and model this dataset with a multivariate linear regression to predict the sale price of houses as accurately as possible.\n",
    "\n",
    "* Student name: Caitlin Snyder\n",
    "* Student pace: self-paced\n",
    "* Scheduled project review date/time: \n",
    "* Instructor name: Jeff Herman\n",
    "* Blog post URL: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I wrap up Module 3 of Flat Iron's Data Science bootcamp, I will be tacking a Driven Data competition, [Pump It Up: Data Mining the Water Table](https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/page/25/).\n",
    "\n",
    "Follow along below, or take a look at the [Jupyter notebook and repo on GitHub](https://github.com/caitsnyder/module_3_project).\n",
    "\n",
    "The competition provides a dataset of waterpoints in Tanzania and their associated characteristics. It is our job to predict, using the supplied training labels, whether a pump is functional, non-functional, or functional but in need of repair.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below, I'll be building a model to predict waterpump status given a testing dataset. Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics._plot.confusion_matrix import plot_confusion_matrix\n",
    "from sklearn.utils import resample\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Define the relevant classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll be taking an object-oriented approach to this project and will begin by defining the classes and constants I'll need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Accessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths\n",
    "We'll define our path strings within a simple dictionary for easy loading:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {\n",
    "        'train_values': 'data/training_set_values.csv',\n",
    "        'train_labels': 'data/training_set_labels.csv',\n",
    "        'test_values': 'data/test_set_values.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader\n",
    "The Data Loader will load the appropriate csvs. This helper class includes an option (run_type_dev) to downsample our dataset as needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def load(self, outcome, run_type_dev, sample_size):\n",
    "        X_train = self.load_from_path(paths['train_values'])\n",
    "        y_train = self.load_from_path(paths['train_labels'])\n",
    "        \n",
    "        if run_type_dev:\n",
    "            print(f\"Sample size = {sample_size}\")\n",
    "            X_train = X_train.iloc[0:sample_size]\n",
    "            y_train = y_train.iloc[0:sample_size]\n",
    "            \n",
    "        df = pd.concat([X_train, y_train], axis=1, join=\"inner\")\n",
    "        self.outcome_values = np.unique(df[outcome])\n",
    "        return df\n",
    "    \n",
    "    def load_from_path(self, path):\n",
    "        df = pd.read_csv(path)\n",
    "        df.set_index('id', inplace=True)\n",
    "        return df\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VizHelper\n",
    "The Viz Helper will output relevant visualizations to inform iterative cleaning and analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VizHelper:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def show_visualizations(self, df, outcome):\n",
    "        cont_features = df.select_dtypes(exclude=['object']).columns\n",
    "        self.check_outcome_distribution(df, outcome)\n",
    "        self.generate_heat_map(df, cont_features)\n",
    "        self.show_outliers(df, cont_features)\n",
    "        self.show_basic_correlations(df, cont_features, outcome)\n",
    "        self.show_outcome_dist(df, outcome)\n",
    "\n",
    "    def check_outcome_distribution(self, df, outcome):\n",
    "        labels = df[outcome].value_counts().index\n",
    "        cnts = df[outcome].value_counts().values\n",
    "        \n",
    "        df_temp = pd.DataFrame({'labels':labels, 'counts':cnts})\n",
    "        ax = df_temp.plot.bar(x='labels', y='counts', rot=0)\n",
    "        ax.set_title(\"Frequency of outcome values\")\n",
    "\n",
    "    def generate_heat_map(self, df, features):\n",
    "        plt.figure(figsize=(7, 6))\n",
    "        sns.heatmap(df[features].corr(), center=0)\n",
    "        plt.show()\n",
    "\n",
    "    def show_outliers(self, df, cols):\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(9, 6))\n",
    "        axe = axes.ravel()\n",
    "\n",
    "        for i, xcol in enumerate(cols):\n",
    "            sns.boxplot(x=df[xcol], ax=axe[i])\n",
    "        plt.show()\n",
    "\n",
    "    def show_basic_correlations(self, df, cols, outcome):\n",
    "        preds = [i for i in cols if i != outcome]\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(9, 6))\n",
    "        axe = axes.ravel()\n",
    "\n",
    "        for i, xcol in enumerate(preds):\n",
    "            df.plot(kind='scatter', x=xcol, y=outcome, alpha=0.4, color='b', ax=axe[i])\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    def show_outcome_dist(self, df, outcome):\n",
    "        df[outcome].value_counts().plot(kind='bar')\n",
    "\n",
    "    def show_confusion_matrix(self, clf, X_test, y_test, outcome, title):\n",
    "        labels = y_test[outcome].unique()\n",
    "        disp = plot_confusion_matrix(clf, X_test, y_test,\n",
    "                                        display_labels=labels,\n",
    "                                        cmap=plt.cm.Greens,\n",
    "                                        xticks_rotation='vertical')\n",
    "        disp.ax_.set_title(title)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaner\n",
    "Our Cleaner will perform basic cleaning tasks (eliminate impossible 0 values, correct column data types) on our raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cleaner():\n",
    "    def __init__(self):\n",
    "        self.viz_helper = VizHelper()\n",
    "\n",
    "    def clean_df(self, df):\n",
    "        df.drop(['num_private', 'amount_tsh'], axis=1, inplace=True)\n",
    "        self.convert_to_string(df)\n",
    "        self.replace_nan(df)\n",
    "        self.bin_date(df)\n",
    "        self.bin_categorical_features(df)\n",
    "        return df\n",
    "\n",
    "    def convert_to_string(self, df):\n",
    "        cols = [\n",
    "            'region_code', \n",
    "            'district_code', \n",
    "            'public_meeting', \n",
    "            'permit'\n",
    "        ]\n",
    "        list(map(lambda col: self.change_type(df, col, str), cols))\n",
    "    \n",
    "    def change_type(self, df, col, new_type):\n",
    "        df[col] = df[col].astype(new_type)\n",
    "\n",
    "    def replace_nan(self, df):\n",
    "        self.replace_null_strings(df, \"nan\")\n",
    "        self.replace_null_strings(df, \"none\")\n",
    "        self.replace_zeros(df, \"longitude\")\n",
    "        self.replace_zeros(df, \"latitude\")\n",
    "        self.replace_zeros(df, 'construction_year')\n",
    "        self.replace_zeros(df, 'population')\n",
    "\n",
    "    def replace_null_strings(self, df, null_str):\n",
    "        df.replace(to_replace=null_str, value=\"unknown\", inplace=True)\n",
    "\n",
    "    def replace_zeros(self, df, col):\n",
    "        df[col] = df.apply(lambda row: np.nan \n",
    "            if row[col] == 0 else row[col], axis=1)\n",
    "\n",
    "    def bin_date(self, df):\n",
    "        df['year'] = [x.split(\"-\")[0] for x in df['date_recorded']]\n",
    "        df['month'] = [x.split(\"-\")[1] for x in df['date_recorded']]\n",
    "        df.drop(['date_recorded'], axis=1, inplace=True)\n",
    "\n",
    "    def bin_categorical_features(self, df):\n",
    "        cols = df.select_dtypes(include=['object']).columns.values.tolist()\n",
    "        for col in cols:\n",
    "            top_10 = df[col].value_counts().index[:10].tolist()\n",
    "            if len(top_10) == 10:\n",
    "                df[col] = [x if x in top_10 else \"Other\" for x in df[col]]\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) Build the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splits Manager\n",
    "The Splits Manager will allow us to easily access our train-test datasets (without worrying about the typos that a simple dictionary access command is susceptible to): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitsManager:\n",
    "    def __init__(self):\n",
    "        self.X_train = None\n",
    "        self.X_test = None, \n",
    "        self.y_train = None \n",
    "        self.y_test = None\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processor\n",
    "The Pre-Processor defines transformations our pipeline will use. Since our dataset is unbalanced (a histogram of our outcome variable shows that 'functional' is vastly over-represented in the dataset relative to the other two outcomes), we will oversample our minority outcomes to achieve a more balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessor():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_preprocessor(self, df):\n",
    "        return {\n",
    "            'sampler': self.get_resampler(),\n",
    "            'col': self.get_col_transformer(df)\n",
    "            }\n",
    "\n",
    "    def get_resampler(self):\n",
    "        return RandomOverSampler(random_state=42)\n",
    "        \n",
    "    def get_col_transformer(self, df):\n",
    "        cont_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())])\n",
    "\n",
    "        cat_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "        return ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', cont_transformer, self.get_cont_features(df)),\n",
    "                ('cat', cat_transformer, self.get_cat_features(df))])\n",
    "\n",
    "    def get_cat_features(self, df):\n",
    "        return df.select_dtypes(include=['object']).columns\n",
    "        \n",
    "    def get_cont_features(self, df):\n",
    "        return df.select_dtypes(exclude=['object']).columns\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers\n",
    "We can store our classifiers in a dictionary so that we can easily iterate over them during analysis:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'decision_tree': DecisionTreeClassifier(),\n",
    "    'random_forest': RandomForestClassifier(),\n",
    "#     'svm': SVC(),\n",
    "    'knn': KNeighborsClassifier(),\n",
    "    'xgboost': XGBClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Param Grids\n",
    "We can similarly store the corresponding param grid for each classifier in a dictionary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'decision_tree': {\n",
    "        'clf__criterion': ['gini', 'entropy'],\n",
    "        'clf__max_depth': [None, 2, 3, 4, 5, 6],\n",
    "        'clf__min_samples_split': [2, 5, 10],\n",
    "        'clf__min_samples_leaf': [1, 2, 3, 4, 5, 6]\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'clf__n_estimators': [10, 20, 30],\n",
    "        'clf__criterion': ['gini', 'entropy'],\n",
    "        'clf__max_depth': [5, 10, 20, 30, 35],\n",
    "        'clf__min_samples_split': [2, 5, 10],\n",
    "        'clf__min_samples_leaf': [2, 3, 6]\n",
    "    },\n",
    "    'svm': {\n",
    "        'clf__C': [0.1, 1, 10], \n",
    "        'clf__gamma': [1, 0.1, 0.01],\n",
    "        'clf__kernel': ['rbf']\n",
    "    },\n",
    "    'knn': {\n",
    "        'clf__n_neighbors': [3, 5, 7, 11, 19],\n",
    "        'clf__weights': ['uniform', 'distance'],\n",
    "        'clf__metric': ['euclidean', 'manhattan']\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'clf__max_depth': [10],\n",
    "        # 'clf__min_child_weight': [1],\n",
    "        # 'clf__eta': [.3],\n",
    "        # 'clf__subsample': [1],\n",
    "        # 'clf__colsample_bytree': [1],\n",
    "        # 'clf__objective': ['reg:linear'],\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Manager\n",
    "The Report Manager will use store and organize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultsManager:\n",
    "    def __init__(self, key, results):\n",
    "        self.clf_name = key.replace(\"_\", \" \").upper()\n",
    "        self.best_estimator = results.best_estimator_\n",
    "        self.best_score = results.best_score_\n",
    "        self.best_params = results.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report Manager\n",
    "The Report Manager will use the shared keys in these two dictionaries to iterate over each classifier and score its performance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReportManager:\n",
    "    def __init__(self, outcome):\n",
    "        self.splits = None\n",
    "        self.results = []\n",
    "        self.outcome = outcome\n",
    "\n",
    "\n",
    "    def run_reports(self, preprocessor, splits: SplitsManager):\n",
    "        self.splits = splits\n",
    "        list(map(lambda key: self.execute_pipeline(preprocessor, key), classifiers.keys()))\n",
    "        list(map(lambda x: self.display_predictions(x), self.results))\n",
    "        self.display_results()\n",
    "\n",
    "    def execute_pipeline(self, preprocessor, key):\n",
    "        print(f\"Beginning {key}...\")\n",
    "        param_grid = param_grids[key]\n",
    "        param_grid['col__num__imputer__strategy'] = ['mean', 'median']\n",
    "\n",
    "        pipe = imbPipeline([\n",
    "            ('col', preprocessor['col']),\n",
    "            ('sampler', preprocessor['sampler']),\n",
    "            (\"clf\", classifiers[key])\n",
    "            ])\n",
    "        gs = GridSearchCV(pipe,\n",
    "                            param_grid,\n",
    "                            cv=3,\n",
    "                            scoring=\"accuracy\",\n",
    "                            n_jobs=-1\n",
    "                        )\n",
    "        gs_results  = gs.fit(self.splits.X_train, self.splits.y_train.values.ravel())\n",
    "        self.results.append(ResultsManager(key, gs_results))\n",
    "\n",
    "    def display_results(self):\n",
    "        rows = list(map(lambda x: [x.clf_name, x.best_score, x.best_params], self.results))\n",
    "        df = pd.DataFrame(rows, columns=[\n",
    "            'clf_name', \n",
    "            'best_score', \n",
    "            'best_params'\n",
    "        ]).sort_values(by='best_score', ascending = False)\n",
    "        print(df)\n",
    "        print('---------\\nAccuracy reports\\n---------')\n",
    "        for result in self.results:\n",
    "            print(f\"{result.clf_name}: {result.best_score:.2%}\")\n",
    "\n",
    "    def display_predictions(self, result: ResultsManager):\n",
    "        title = f\"{result.clf_name} ({result.best_score:.2%})\"\n",
    "        VizHelper().show_confusion_matrix(result.best_estimator, self.splits.X_test, self.splits.y_test, self.outcome, title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D) Tying it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Manager\n",
    "The Data Manager is responsible for coordinating the different helper classes responsible for cleaning, pipeline-creation, visualization, and analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "    def __init__(self, sample_size, run_type_dev=True):\n",
    "\n",
    "        self.sample_size = sample_size\n",
    "        self.outcome = 'status_group'\n",
    "        self.splits = SplitsManager()\n",
    "        self.process_data(run_type_dev)\n",
    "\n",
    "    def process_data(self, run_type_dev):\n",
    "        raw_df = DataLoader().load(self.outcome, run_type_dev, self.sample_size)\n",
    "        df = Cleaner().clean_df(raw_df)\n",
    "        VizHelper().show_visualizations(df, self.outcome)\n",
    "        self.split_data(df, self.outcome)\n",
    "   \n",
    "    def split_data(self, df, outcome):\n",
    "        X = df[[i for i in df.columns if i != outcome]]\n",
    "        y = df[[outcome]]\n",
    "        \n",
    "        self.splits.X_train, self.splits.X_test,\\\n",
    "            self.splits.y_train, self.splits.y_test =\\\n",
    "            tts(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "    def get_report(self):\n",
    "        preprocessor = PreProcessor().get_preprocessor(self.splits.X_train)\n",
    "        ReportManager(self.outcome).run_reports(preprocessor, self.splits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined our classes, we can kick off the analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = DataManager(sample_size=500, run_type_dev=False)\n",
    "dfs.get_report()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Results & Take-aways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's first consider the implications of our exploratory visualizations:\n",
    "\n",
    "1) Our predictors are not highly correlated. We do not need to drop features out of concerns for multicollinearity.\n",
    "\n",
    "2) Our outcome variables is extremely unbalanced. We will compensate for this by oversampling the underrepresented classes in our pipeline.\n",
    "\n",
    "3) Our boxplots indicate that our predictors are fairly free of outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning to our confusion matrices, let's review the relative performance of our different classifiers:\n",
    "\n",
    "1) Both our Decision Tree and K-Nearest Neighbors classifiers performed well (73.7% and 75.1% accuracy, respectively).\n",
    "\n",
    "2) Random Forest and XGBoost performed best, with XGBoost (77.6%) taking a slight lead over Random Forest (77.0%).\n",
    "\n",
    "3) Support Vector Machine could not be completed locally with the available computing resources. In future projects, I will train my models using cloud-based servers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional avenues for exploration\n",
    "\n",
    "In the future, it would be exciting to consider how frequency of conflict incidences related to water-resource usage might impact the model's performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://towardsdatascience.com/fuzzywuzzy-find-similar-strings-within-one-column-in-a-pandas-data-frame-99f6c2a0c212\n",
    "    \n",
    "- https://towardsdatascience.com/fuzzywuzzy-fuzzy-string-matching-in-python-beginners-guide-9adc0edf4b35\n",
    "\n",
    "- https://stackabuse.com/overview-of-classification-methods-in-python-with-scikit-learn/\n",
    "    \n",
    "- https://stackabuse.com/the-naive-bayes-algorithm-in-python-with-scikit-learn/\n",
    "\n",
    "- https://medium.com/@erikgreenj/k-neighbors-classifier-with-gridsearchcv-basics-3c445ddeb657\n",
    "\n",
    "- https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf\n",
    "\n",
    "- https://datascience.stackexchange.com/questions/60862/if-i-have-negative-and-positive-numbers-for-a-feature-should-minmaxscaler-be-1\n",
    "\n",
    "- https://stackoverflow.com/questions/41899132/invalid-parameter-for-sklearn-estimator-pipeline\n",
    "\n",
    "- https://lifewithdata.com/2021/04/02/how-to-build-machine-learning-pipeline-with-scikit-learn-and-why-is-it-essential/\n",
    "\n",
    "- https://stackoverflow.com/questions/63467815/how-to-access-columntransformer-elements-in-gridsearchcv\n",
    "\n",
    "- https://openscoring.io/blog/2020/10/24/converting_sklearn_imblearn_pipeline_pmml/\n",
    "\n",
    "- https://imbalanced-learn.org/stable/over_sampling.html\n",
    "\n",
    "- https://towardsdatascience.com/imbalanced-class-sizes-and-classification-models-a-cautionary-tale-part-2-cf371500d1b3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
